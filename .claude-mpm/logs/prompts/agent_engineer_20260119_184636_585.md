---
timestamp: 2026-01-19T18:46:36.585643+00:00
type: agent_engineer
metadata: {"agent_type": "engineer", "agent_id": "engineer_4022cdf5-d9e6-4ed3-a6c0-704bd03066aa", "session_id": "4022cdf5-d9e6-4ed3-a6c0-704bd03066aa", "delegation_context": {"description": "Implement R7 Chat API and RAG Pipeline", "timestamp": "2026-01-19T18:46:36.582709+00:00"}}
---


AGENT MEMORY - PROJECT-SPECIFIC KNOWLEDGE:
# Agent Memory: engineer

<!-- Last Updated: 2026-01-19T15:48:05.096598+00:00Z -->


INSTRUCTIONS: Review your memory above before proceeding. Apply learned patterns and avoid known mistakes.


## Task: Implement R7 - Chat API and RAG Pipeline

Implement the chat interface backend for article-mind-service following the detailed plan.

### Plan Location
Read and follow: /export/workspace/article-mind/docs/plans/plan-R7-chat-interface.md

### Architecture Decisions (Pre-determined)
- LLM: Dual mode (GPT-4o-mini for cost, Claude for quality)
- Search: Use R6 hybrid search for retrieval
- Persistence: Store Q&A history in PostgreSQL

### Key Requirements

1. **Dependencies** - Add to pyproject.toml:
   - openai (already added for embeddings)
   - anthropic (for Claude)

2. **Database Schema** - Create `ChatMessage` model:
   ```python
   class ChatMessage(Base):
       __tablename__ = "chat_messages"
       
       id: Mapped[int] = mapped_column(primary_key=True)
       session_id: Mapped[int] = mapped_column(ForeignKey("research_sessions.id"))
       role: Mapped[str]  # "user" | "assistant"
       content: Mapped[str]
       sources: Mapped[dict | None] = mapped_column(JSON)  # [{article_id, chunk_id, title, url}]
       created_at: Mapped[datetime]
   ```

3. **Module Structure** - Create `src/article_mind_service/chat/`:
   ```
   chat/
   ├── __init__.py
   ├── llm_providers.py    # LLMProvider ABC, OpenAI, Anthropic implementations
   ├── rag_pipeline.py     # Retrieve → Augment → Generate
   ├── prompts.py          # System prompts for RAG
   └── exceptions.py       # Custom exceptions
   ```

4. **LLM Provider Abstraction**:
   ```python
   class LLMProvider(ABC):
       @abstractmethod
       async def generate(
           self,
           system_prompt: str,
           user_message: str,
           context_chunks: list[str]
       ) -> str:
           pass
   
   class OpenAIProvider(LLMProvider):
       # GPT-4o-mini implementation
   
   class AnthropicProvider(LLMProvider):
       # Claude Sonnet implementation
   ```

5. **RAG Pipeline**:
   ```
   User Query → Search (R6) → Build Context → LLM Generate → Extract Citations → Return
   ```
   - Use R6 search to retrieve relevant chunks
   - Format context with numbered sources [1], [2], etc.
   - Generate answer with LLM
   - Parse citations from response

6. **System Prompt**:
   ```
   You are a research assistant. Answer questions based ONLY on the 
   provided context. Cite sources using [1], [2] format.
   
   If the context doesn't contain relevant information, say so clearly.
   
   Context:
   [1] {chunk_1_content} (Source: {title})
   [2] {chunk_2_content} (Source: {title})
   ...
   ```

7. **Pydantic Schemas** - Create in `schemas/chat.py`:
   ```python
   class ChatRequest(BaseModel):
       message: str
   
   class ChatSource(BaseModel):
       article_id: int
       title: str | None
       url: str | None
   
   class ChatResponse(BaseModel):
       message_id: int
       content: str
       sources: list[ChatSource]
       created_at: datetime
   
   class ChatHistoryResponse(BaseModel):
       messages: list[ChatMessageResponse]
   ```

8. **API Endpoints** - Create in `routers/chat.py`:
   - POST /api/v1/sessions/{session_id}/chat - Send message, get response
   - GET /api/v1/sessions/{session_id}/chat/history - Get chat history
   - DELETE /api/v1/sessions/{session_id}/chat/history - Clear history

9. **Configuration** (.env):
   ```
   LLM_PROVIDER=openai  # or anthropic
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-...
   LLM_MODEL=gpt-4o-mini  # or claude-sonnet-4-5-20241022
   LLM_MAX_TOKENS=2048
   RAG_CONTEXT_CHUNKS=5
   ```

### Working Directory
/export/workspace/article-mind/article-mind-service/

### Quality Requirements
- All code passes: make lint, make typecheck
- Unit tests for LLM providers with mocked responses
- Integration test for chat flow

Implement the complete R7 chat backend, run quality checks, and report results.